# gRPC Transformations - Detaillierte Technische Spezifikation

**Feature:** v1.4.0 Feature 1 - gRPC Transformations
**Version:** 1.0
**Status:** Spezifikation (vor Implementierung)
**Autor:** Claude Code
**Datum:** 2025-10-19

---

## 1. Übersicht

### 1.1 Ziel

Ermögliche Benutzern die Transformation von gRPC Request/Response-Nachrichten mithilfe von Protobuf-Descriptors. Biete eine provider-agnostische Konfiguration für:

- **Request-Transformationen**: Felder hinzufügen (Trace-IDs, Timestamps), entfernen (Secrets), umbenennen
- **Response-Transformationen**: Felder filtern (Passwörter), hinzufügen (Metadaten)
- **Proto-Descriptor-Management**: .proto-Dateien laden (file, inline, URL)
- **Multi-Provider-Support**: Envoy, Nginx, APISIX, Kong, HAProxy, Traefik

### 1.2 Nicht-Ziele (Out of Scope)

- ❌ gRPC ↔ REST Transcoding (bereits in APISIX/Kong vorhanden)
- ❌ Protobuf Schema-Evolution (Breaking Changes)
- ❌ gRPC Streaming-Transformationen (nur Unary RPCs)
- ❌ Dynamische Proto-Reflection (nur file-basierte Descriptors)
- ❌ Custom Protobuf Extensions

### 1.3 Use Cases

1. **Trace-ID Injection**: Füge eindeutige Request-IDs für Distributed Tracing hinzu
2. **Secret Removal**: Entferne sensible Felder (API Keys, Tokens) vor Backend-Aufruf
3. **Field Renaming**: Snake_case ↔ camelCase Konvertierung zwischen Client/Server
4. **Response Filtering**: Entferne interne Felder (password_hash, internal_id) vor Client-Antwort
5. **Metadata Addition**: Füge Server-Metadaten (Timestamps, Gateway-Version) zu Responses hinzu

---

## 2. API Design

### 2.1 Config Model Extensions (gal/config.py)

#### 2.1.1 ProtoDescriptor Dataclass

```python
from dataclasses import dataclass, field
from typing import Optional

@dataclass
class ProtoDescriptor:
    """Protobuf-Descriptor-Konfiguration für gRPC-Services.

    Attributes:
        name: Eindeutiger Descriptor-Name (wird als Referenz verwendet)
        source: Quelle des Proto-Files ("file", "inline", "url")
        path: Dateipfad zur .proto oder .desc Datei (bei source="file")
        content: Inline Proto-Definition (bei source="inline")
        url: Download-URL für Proto-Datei (bei source="url")

    Examples:
        >>> # File-based descriptor
        >>> desc = ProtoDescriptor(
        ...     name="user_service",
        ...     source="file",
        ...     path="/etc/gal/protos/user.desc"
        ... )

        >>> # Inline descriptor
        >>> desc = ProtoDescriptor(
        ...     name="order_service",
        ...     source="inline",
        ...     content='syntax = "proto3"; ...'
        ... )

        >>> # URL-based descriptor
        >>> desc = ProtoDescriptor(
        ...     name="payment_service",
        ...     source="url",
        ...     url="https://api.example.com/protos/payment.proto"
        ... )

    Validation:
        - name: Required, non-empty, unique
        - source: Must be one of ["file", "inline", "url"]
        - path: Required if source="file", must exist
        - content: Required if source="inline", must be valid proto3 syntax
        - url: Required if source="url", must be valid HTTP(S) URL
    """
    name: str
    source: str  # "file", "inline", "url"
    path: str = ""
    content: str = ""
    url: str = ""

    def __post_init__(self):
        """Validate descriptor configuration."""
        if not self.name:
            raise ValueError("ProtoDescriptor.name is required")

        if self.source not in ["file", "inline", "url"]:
            raise ValueError(f"Invalid source: {self.source}. Must be 'file', 'inline', or 'url'")

        if self.source == "file" and not self.path:
            raise ValueError("ProtoDescriptor.path is required when source='file'")

        if self.source == "inline" and not self.content:
            raise ValueError("ProtoDescriptor.content is required when source='inline'")

        if self.source == "url" and not self.url:
            raise ValueError("ProtoDescriptor.url is required when source='url'")
```

#### 2.1.2 GrpcTransformation Dataclass

```python
@dataclass
class GrpcTransformation:
    """gRPC-spezifische Transformation-Konfiguration.

    Attributes:
        enabled: Enable/Disable gRPC transformation
        proto_descriptor: Referenz zum ProtoDescriptor-Namen
        package: Protobuf-Package (z.B. "user.v1", "order.api")
        service: Service-Name aus Proto (z.B. "UserService")
        request_type: Request-Message-Type (z.B. "CreateUserRequest")
        response_type: Response-Message-Type (z.B. "CreateUserResponse")
        request_transform: Request-Transformationen (add, remove, rename)
        response_transform: Response-Transformationen (filter, add)

    Examples:
        >>> transform = GrpcTransformation(
        ...     enabled=True,
        ...     proto_descriptor="user_service",
        ...     package="user.v1",
        ...     service="UserService",
        ...     request_type="CreateUserRequest",
        ...     response_type="CreateUserResponse",
        ...     request_transform=RequestBodyTransformation(
        ...         add_fields={"trace_id": "{{uuid}}"},
        ...         remove_fields=["internal_secret"]
        ...     ),
        ...     response_transform=ResponseBodyTransformation(
        ...         filter_fields=["password_hash"]
        ...     )
        ... )

    Validation:
        - proto_descriptor: Must reference existing ProtoDescriptor
        - package: Required, must match proto package declaration
        - service: Required, must exist in proto
        - request_type: Required, must exist in proto
        - response_type: Required, must exist in proto
    """
    enabled: bool = True
    proto_descriptor: str = ""
    package: str = ""
    service: str = ""
    request_type: str = ""
    response_type: str = ""
    request_transform: Optional["RequestBodyTransformation"] = None
    response_transform: Optional["ResponseBodyTransformation"] = None

    def __post_init__(self):
        """Validate gRPC transformation configuration."""
        if self.enabled:
            if not self.proto_descriptor:
                raise ValueError("proto_descriptor is required when enabled=True")
            if not self.package:
                raise ValueError("package is required when enabled=True")
            if not self.service:
                raise ValueError("service is required when enabled=True")
            if not self.request_type:
                raise ValueError("request_type is required when enabled=True")
            if not self.response_type:
                raise ValueError("response_type is required when enabled=True")
```

#### 2.1.3 Route Extension

```python
@dataclass
class Route:
    # ... existing fields ...
    grpc_transformation: Optional[GrpcTransformation] = None
```

#### 2.1.4 Config Extension

```python
@dataclass
class Config:
    # ... existing fields ...
    proto_descriptors: List[ProtoDescriptor] = field(default_factory=list)
```

### 2.2 ProtoManager Class (gal/proto_manager.py)

#### 2.2.1 Class Signature

```python
import os
import subprocess
import hashlib
import requests
from pathlib import Path
from typing import Dict, Optional, List
from gal.config import ProtoDescriptor

class ProtoManager:
    """Protobuf-Descriptor-Manager für gRPC-Transformationen.

    Verwaltet Proto-Descriptors:
    - Lädt .proto-Dateien von Disk, inline oder URL
    - Kompiliert .proto → .desc mit protoc
    - Cached kompilierte Descriptors
    - Validiert Proto-Syntax

    Attributes:
        proto_dir: Verzeichnis für Proto-Dateien (default: /etc/gal/protos)
        descriptors: Registry aller geladenen Descriptors

    Thread-Safety: Nicht thread-safe (single-threaded usage)
    """

    def __init__(self, proto_dir: str = "/etc/gal/protos"):
        """Initialize ProtoManager.

        Args:
            proto_dir: Directory to store proto files and compiled descriptors

        Raises:
            OSError: If proto_dir cannot be created
        """
        self.proto_dir = Path(proto_dir)
        self.descriptors: Dict[str, ProtoDescriptor] = {}
        self.proto_dir.mkdir(parents=True, exist_ok=True)

    def register_descriptor(self, descriptor: ProtoDescriptor) -> None:
        """Registriere Proto-Descriptor und kompiliere falls nötig.

        Args:
            descriptor: ProtoDescriptor-Konfiguration

        Raises:
            FileNotFoundError: Wenn source="file" und path nicht existiert
            ValueError: Wenn Proto-Syntax ungültig
            RuntimeError: Wenn protoc-Kompilierung fehlschlägt
            requests.HTTPError: Wenn URL-Download fehlschlägt

        Side Effects:
            - Erstellt .proto-Datei in proto_dir (bei inline/url)
            - Kompiliert .proto → .desc
            - Updated descriptor.path zu .desc-Pfad
        """
        # Validation bereits in __post_init__
        self.descriptors[descriptor.name] = descriptor

        if descriptor.source == "file":
            self._validate_file(descriptor)
        elif descriptor.source == "inline":
            self._process_inline(descriptor)
        elif descriptor.source == "url":
            self._process_url(descriptor)

    def get_descriptor(self, name: str) -> Optional[ProtoDescriptor]:
        """Hole registrierten Descriptor nach Namen.

        Args:
            name: Descriptor-Name

        Returns:
            ProtoDescriptor or None if not found
        """
        return self.descriptors.get(name)

    def list_descriptors(self) -> List[str]:
        """Liste alle registrierten Descriptor-Namen.

        Returns:
            Liste von Descriptor-Namen
        """
        return list(self.descriptors.keys())

    def _validate_file(self, descriptor: ProtoDescriptor) -> None:
        """Validiere file-based descriptor.

        Args:
            descriptor: ProtoDescriptor mit source="file"

        Raises:
            FileNotFoundError: Wenn path nicht existiert
            ValueError: Wenn path nicht .proto oder .desc ist
        """
        path = Path(descriptor.path)
        if not path.exists():
            raise FileNotFoundError(f"Proto file not found: {descriptor.path}")

        if path.suffix not in [".proto", ".desc"]:
            raise ValueError(f"Invalid proto file extension: {path.suffix}")

        # Wenn .proto, kompiliere zu .desc
        if path.suffix == ".proto":
            descriptor.path = str(self._compile_proto(str(path)))

    def _process_inline(self, descriptor: ProtoDescriptor) -> None:
        """Verarbeite inline-Proto-Content.

        Args:
            descriptor: ProtoDescriptor mit source="inline"

        Raises:
            RuntimeError: Wenn protoc-Kompilierung fehlschlägt

        Side Effects:
            - Schreibt .proto-Datei zu proto_dir/{name}.proto
            - Kompiliert zu .desc
            - Updated descriptor.path
        """
        # Hash content für eindeutigen Dateinamen (bei Content-Updates)
        content_hash = hashlib.md5(descriptor.content.encode()).hexdigest()[:8]
        proto_file = self.proto_dir / f"{descriptor.name}_{content_hash}.proto"

        # Schreibe Proto-Content
        proto_file.write_text(descriptor.content)

        # Kompiliere
        desc_file = self._compile_proto(str(proto_file))
        descriptor.path = str(desc_file)

    def _process_url(self, descriptor: ProtoDescriptor) -> None:
        """Lade Proto-Datei von URL und kompiliere.

        Args:
            descriptor: ProtoDescriptor mit source="url"

        Raises:
            requests.HTTPError: Wenn Download fehlschlägt
            RuntimeError: Wenn protoc-Kompilierung fehlschlägt

        Side Effects:
            - Downloaded .proto zu proto_dir/{name}.proto
            - Kompiliert zu .desc
            - Updates descriptor.path
        """
        proto_file = self.proto_dir / f"{descriptor.name}.proto"

        # Download mit Timeout
        response = requests.get(descriptor.url, timeout=30)
        response.raise_for_status()

        # Schreibe Proto-Content
        proto_file.write_bytes(response.content)

        # Kompiliere
        desc_file = self._compile_proto(str(proto_file))
        descriptor.path = str(desc_file)

    def _compile_proto(self, proto_file: str) -> Path:
        """Kompiliere .proto zu .desc mit protoc.

        Args:
            proto_file: Pfad zur .proto-Datei

        Returns:
            Path zum kompilierten .desc-File

        Raises:
            RuntimeError: Wenn protoc nicht installiert oder Kompilierung fehlschlägt

        Command:
            protoc --descriptor_set_out={desc_file} \\
                   --proto_path={proto_dir} \\
                   --include_imports \\
                   {proto_file}
        """
        proto_path = Path(proto_file)
        desc_file = proto_path.with_suffix(".desc")

        # protoc command
        cmd = [
            "protoc",
            f"--descriptor_set_out={desc_file}",
            f"--proto_path={self.proto_dir}",
            "--include_imports",  # Include dependencies
            str(proto_file)
        ]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=False,
                timeout=30
            )
        except FileNotFoundError:
            raise RuntimeError(
                "protoc not found. Install with: apt-get install protobuf-compiler"
            )
        except subprocess.TimeoutExpired:
            raise RuntimeError(f"protoc compilation timeout for {proto_file}")

        if result.returncode != 0:
            raise RuntimeError(
                f"protoc compilation failed for {proto_file}:\n{result.stderr}"
            )

        return desc_file
```

---

## 3. Provider-Implementierungen

### 3.1 Envoy Provider (Lua Filter)

#### 3.1.1 Implementation Strategy

**Ansatz:** Envoy HTTP Lua Filter mit lua-protobuf Library

**Dependencies:**
- Envoy mit Lua Filter compiled
- lua-protobuf library im Lua-Path

**Generierte Config:**
```yaml
http_filters:
  - name: envoy.filters.http.lua
    typed_config:
      "@type": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua
      inline_code: |
        -- Lua code hier
```

#### 3.1.2 Lua Code Template

```lua
-- Proto Descriptor (global, einmal geladen)
local pb = require("pb")
local _proto_loaded = false

function envoy_on_request(request_handle)
    -- Proto Descriptor laden (lazy loading)
    if not _proto_loaded then
        pb.load(io.open("{desc_path}", "rb"):read("*all"))
        _proto_loaded = true
    end

    -- gRPC Message Body abrufen
    local body = request_handle:body()
    if not body or body:length() == 0 then
        return
    end

    -- Protobuf dekodieren (5-Byte gRPC Header überspringen)
    local grpc_header = body:getBytes(0, 5)
    local pb_bytes = body:getBytes(5, body:length() - 5)
    local msg = pb.decode("{request_type}", pb_bytes)

    if not msg then
        request_handle:logErr("Failed to decode gRPC message")
        return
    end

    -- Transformationen anwenden
    {transformation_code}

    -- Zurück enkodieren
    local new_pb = pb.encode("{request_type}", msg)
    local new_body = grpc_header .. new_pb
    request_handle:body():setBytes(new_body)
end

function envoy_on_response(response_handle)
    -- Similar für Response
end
```

#### 3.1.3 Transformation Code Generation

```python
def _generate_request_transform_lua(self, transform: RequestBodyTransformation) -> str:
    """Generiere Lua-Code für Request-Transformationen.

    Args:
        transform: RequestBodyTransformation config

    Returns:
        Lua code string
    """
    lua_lines = []

    # Add fields
    if transform.add_fields:
        lua_lines.append("-- Add fields")
        for key, value in transform.add_fields.items():
            if value == "{{uuid}}":
                # Envoy request ID
                lua_lines.append(f'msg["{key}"] = request_handle:headers():get("x-request-id")')
            elif value in ["{{timestamp}}", "{{now}}"]:
                lua_lines.append(f'msg["{key}"] = os.time()')
            else:
                # Static value
                lua_lines.append(f'msg["{key}"] = "{value}"')

    # Remove fields
    if transform.remove_fields:
        lua_lines.append("-- Remove fields")
        for field in transform.remove_fields:
            lua_lines.append(f'msg["{field}"] = nil')

    # Rename fields
    if transform.rename_fields:
        lua_lines.append("-- Rename fields")
        for old_name, new_name in transform.rename_fields.items():
            lua_lines.append(f'msg["{new_name}"] = msg["{old_name}"]')
            lua_lines.append(f'msg["{old_name}"] = nil')

    return "\n    ".join(lua_lines)
```

#### 3.1.4 Edge Cases

1. **gRPC Header Handling**: 5-Byte Header (Compression Flag + Message Length)
   - Solution: Preserve header, transform only payload

2. **Large Messages**: > 4MB Body Buffer
   - Solution: Log warning, skip transformation

3. **Invalid Protobuf**: Decode fails
   - Solution: Log error, forward original message

4. **Missing Descriptor**: Proto file not found
   - Solution: Fail-fast at config validation

### 3.2 Nginx Provider (OpenResty Lua)

#### 3.2.1 Implementation Strategy

**Ansatz:** OpenResty with lua-resty-protobuf or lua-protobuf

**Dependencies:**
- OpenResty (Nginx + LuaJIT)
- lua-protobuf (luarocks install lua-protobuf)

**Generierte Config:**
```nginx
location /user.v1.UserService/ {
    access_by_lua_block {
        -- Request transformation
    }

    body_filter_by_lua_block {
        -- Response transformation
    }

    grpc_pass grpc://backend;
}
```

#### 3.2.2 Lua Code Template

```lua
-- access_by_lua_block (Request)
local pb = require("pb")
pb.load(io.open("{desc_path}", "rb"):read("*all"))

ngx.req.read_body()
local body = ngx.req.get_body_data()

if body then
    -- Skip 5-byte gRPC header
    local grpc_header = body:sub(1, 5)
    local pb_data = body:sub(6)

    local msg = pb.decode("{request_type}", pb_data)
    if msg then
        {transformation_code}

        local new_pb = pb.encode("{request_type}", msg)
        ngx.req.set_body_data(grpc_header .. new_pb)
    end
end
```

#### 3.2.3 Special Nginx Features

1. **ngx.var.request_id**: Built-in UUID
2. **ngx.time() / ngx.now()**: Timestamp functions
3. **ngx.req.set_body_data()**: Efficient body replacement
4. **Multiple Lua Phases**: access_by, body_filter_by, header_filter_by

### 3.3 APISIX Provider (Serverless Plugin)

#### 3.3.1 Implementation Strategy

**Ansatz:** APISIX serverless-pre-function Plugin

**Generated Config:**
```yaml
plugins:
  serverless-pre-function:
    phase: rewrite
    functions:
      - "return function(conf, ctx) ... end"
```

#### 3.3.2 Lua Function Template

```lua
return function(conf, ctx)
    local pb = require("pb")
    local core = require("apisix.core")

    -- Load proto
    pb.load(io.open("{desc_path}", "rb"):read("*all"))

    -- Get request body
    local body, err = core.request.get_body()
    if not body then
        return
    end

    -- Decode (skip gRPC header)
    local grpc_header = body:sub(1, 5)
    local pb_data = body:sub(6)
    local msg = pb.decode("{request_type}", pb_data)

    if msg then
        {transformation_code}

        local new_pb = pb.encode("{request_type}", msg)
        ngx.req.set_body_data(grpc_header .. new_pb)
    end
end
```

### 3.4 Kong Provider (Plugin Warning)

**Strategy:** Show helpful warning with alternatives

```python
def _generate_grpc_transformation_kong(self, route):
    if not route.grpc_transformation or not route.grpc_transformation.enabled:
        return None

    logger.warning(
        "gRPC Transformation in Kong requires custom plugins or external services.\n"
        "\n"
        "Options:\n"
        "  1. Use Kong's grpc-gateway plugin (gRPC → REST):\n"
        "     https://docs.konghq.com/hub/kong-inc/grpc-gateway/\n"
        "\n"
        "  2. Develop custom Kong plugin with lua-protobuf:\n"
        "     https://docs.konghq.com/gateway/latest/plugin-development/\n"
        "\n"
        "  3. Deploy external transformation service + request-transformer:\n"
        "     - Sidecar service with gRPC interceptor\n"
        "     - Kong's request-transformer to route to sidecar\n"
        "\n"
        "  4. Use alternative provider: Envoy, Nginx, APISIX (native gRPC transform)\n"
    )

    # Basic grpc-gateway config (if user wants REST transcoding)
    return {
        "name": "grpc-gateway",
        "config": {
            "proto": route.grpc_transformation.proto_descriptor,
            "service": route.grpc_transformation.service
        }
    }
```

### 3.5 HAProxy Provider (Lua Script Reference)

**Strategy:** Generate Lua script + instructions

```python
def _generate_grpc_transformation_haproxy(self, route):
    if not route.grpc_transformation or not route.grpc_transformation.enabled:
        return []

    grpc = route.grpc_transformation

    # Generate external Lua script reference
    output = []
    output.append("# gRPC Transformation (requires external Lua script)")
    output.append(f"http-request lua.grpc_transform_{grpc.service}")
    output.append(f"http-response lua.grpc_transform_response_{grpc.service}")

    # Also log setup instructions
    logger.warning(
        "HAProxy gRPC Transformation setup instructions:\n"
        "\n"
        "1. Install lua-protobuf:\n"
        "   luarocks install lua-protobuf\n"
        "\n"
        "2. Create Lua script: /etc/haproxy/lua/grpc_transform.lua\n"
        "\n"
        "3. Add to haproxy.cfg global section:\n"
        "   lua-load /etc/haproxy/lua/grpc_transform.lua\n"
        "\n"
        "4. Lua script template:\n"
        "   -- See docs/guides/GRPC_TRANSFORMATIONS.md\n"
    )

    return output
```

### 3.6 Traefik Provider (Limitation Warning)

**Strategy:** Show clear limitation + alternatives

```python
def _generate_grpc_transformation_traefik(self, route):
    if not route.grpc_transformation or not route.grpc_transformation.enabled:
        return None

    logger.warning(
        "gRPC Transformation is NOT natively supported by Traefik.\n"
        "\n"
        "Traefik Limitations:\n"
        "  ❌ No native Protobuf support\n"
        "  ❌ No Lua/scripting capabilities\n"
        "  ❌ No request/response body transformation\n"
        "\n"
        "Alternatives:\n"
        "\n"
        "  1. Use ForwardAuth Middleware with external gRPC proxy:\n"
        "     - Deploy sidecar gRPC transformation service\n"
        "     - Traefik routes to sidecar\n"
        "     - Sidecar transforms and forwards to backend\n"
        "\n"
        "  2. Develop custom Traefik Plugin (Go):\n"
        "     - Requires Go development\n"
        "     - github.com/traefik/plugindemo\n"
        "\n"
        "  3. **RECOMMENDED:** Use alternative provider:\n"
        "     - Envoy (best gRPC support)\n"
        "     - Nginx/OpenResty (Lua flexibility)\n"
        "     - APISIX (native gRPC transform)\n"
        "     - Kong (with custom plugins)\n"
        "     - HAProxy (with Lua scripts)\n"
    )

    return None
```

---

## 4. Edge Cases und Error Handling

### 4.1 Proto Descriptor Issues

| Issue | Detection | Handling | User Message |
|-------|-----------|----------|--------------|
| Missing .proto file | File existence check | Raise FileNotFoundError | "Proto file not found: {path}" |
| Invalid proto syntax | protoc stderr | Raise RuntimeError | "Invalid proto syntax: {error}" |
| URL download fails | requests.HTTPError | Raise with retry suggestion | "Failed to download proto: {url}" |
| protoc not installed | subprocess.FileNotFoundError | Raise with install instructions | "protoc not found. Install: apt-get install protobuf-compiler" |
| Descriptor name conflict | Dict key check | Raise ValueError | "Descriptor '{name}' already registered" |

### 4.2 Transformation Issues

| Issue | Detection | Handling | User Message |
|-------|-----------|----------|--------------|
| Message decode fails | pb.decode returns nil | Log error, forward original | "Failed to decode gRPC message type {type}" |
| Field not found | Lua table access | Ignore (Lua returns nil) | Warning: "Field '{field}' not found in message" |
| Type mismatch | Proto validation | Log warning | "Field '{field}' type mismatch: expected {type}" |
| Large message (>4MB) | Body length check | Skip transformation | "Message too large for transformation: {size}MB" |
| gRPC header invalid | Header parse check | Forward original | "Invalid gRPC header, skipping transformation" |

### 4.3 Runtime Issues

| Issue | Detection | Handling | Recovery |
|-------|-----------|----------|----------|
| Lua runtime error | pcall wrapper | Log error, forward original | Continue with next request |
| Memory overflow | Lua memory limit | Fail transformation | Increase Lua memory limit |
| CPU timeout | Lua instruction limit | Fail transformation | Optimize transformation logic |
| Proto reload needed | File modification time | Hot-reload descriptor | No downtime |

### 4.4 Configuration Validation

```python
def validate_grpc_transformation(config: Config, proto_manager: ProtoManager) -> List[str]:
    """Validate gRPC transformation configuration.

    Returns:
        List of error messages (empty if valid)
    """
    errors = []

    # Check all proto descriptors are registered
    for service in config.services:
        for route in service.routes:
            if not route.grpc_transformation:
                continue

            grpc = route.grpc_transformation
            if not grpc.enabled:
                continue

            # Check descriptor exists
            descriptor = proto_manager.get_descriptor(grpc.proto_descriptor)
            if not descriptor:
                errors.append(
                    f"Route {route.path_prefix}: "
                    f"Proto descriptor '{grpc.proto_descriptor}' not found. "
                    f"Available: {proto_manager.list_descriptors()}"
                )
                continue

            # Check descriptor file exists
            if not Path(descriptor.path).exists():
                errors.append(
                    f"Route {route.path_prefix}: "
                    f"Proto descriptor file not found: {descriptor.path}"
                )

            # TODO: Validate package, service, types exist in proto
            # (requires proto parsing - future enhancement)

    return errors
```

---

## 5. Performance Considerations

### 5.1 Proto Descriptor Loading

**Problem:** Loading .desc files on every request is expensive

**Solutions:**

1. **Lazy Loading + Caching** (Envoy/Nginx)
   ```lua
   local _proto_cache = {}

   function load_proto(desc_path)
       if not _proto_cache[desc_path] then
           _proto_cache[desc_path] = pb.load(
               io.open(desc_path, "rb"):read("*all")
           )
       end
       return _proto_cache[desc_path]
   end
   ```

2. **Preload at Startup** (APISIX)
   - Load all descriptors in init_worker phase
   - Store in shared dict (ngx.shared.DICT)

3. **File Modification Watching**
   - Detect .desc file changes
   - Hot-reload without restart

### 5.2 Protobuf Encoding/Decoding

**Benchmarks (lua-protobuf):**
- Decode: ~5-10µs per message (1KB)
- Encode: ~10-15µs per message
- Total overhead: ~20-30µs per request

**Optimizations:**
- Use LuaJIT FFI for faster memory access
- Reuse message tables (avoid GC pressure)
- Skip transformation if no fields changed

### 5.3 Memory Usage

**Lua Memory Limits:**
- Envoy: Default 5MB per request
- Nginx: Configurable (lua_max_running_timers)
- APISIX: Shared dict size limits

**Message Size Limits:**
```lua
local MAX_MESSAGE_SIZE = 4 * 1024 * 1024  -- 4MB

if body:length() > MAX_MESSAGE_SIZE then
    request_handle:logWarn(
        string.format("Message too large: %d bytes", body:length())
    )
    return  -- Skip transformation
end
```

---

## 6. Security Considerations

### 6.1 Untrusted Proto Files

**Risk:** Malicious .proto files could:
- Cause protoc to crash
- Include malicious imports
- Generate excessive .desc files

**Mitigations:**
1. **Sandbox protoc execution:**
   ```python
   result = subprocess.run(
       cmd,
       timeout=30,  # Kill after 30s
       capture_output=True,
       cwd=self.proto_dir,  # Restrict to proto_dir
   )
   ```

2. **Validate proto content:**
   - Check file size (< 1MB)
   - Scan for suspicious patterns
   - Whitelist allowed imports

3. **URL source restrictions:**
   ```python
   ALLOWED_PROTO_URLS = [
       "https://buf.build/",
       "https://github.com/",
   ]

   if not any(url.startswith(prefix) for prefix in ALLOWED_PROTO_URLS):
       raise ValueError(f"Proto URL not allowed: {url}")
   ```

### 6.2 Lua Code Injection

**Risk:** User-provided field values could inject Lua code

**Mitigations:**
1. **Escape all user values:**
   ```python
   def escape_lua_string(value: str) -> str:
       return value.replace("\\", "\\\\").replace('"', '\\"')
   ```

2. **Use Lua string literals:**
   ```lua
   -- Bad
   msg["field"] = {user_value}

   -- Good
   msg["field"] = "{escaped_value}"
   ```

3. **No eval() in Lua:**
   - Never use loadstring() or dofile() with user input

### 6.3 Sensitive Field Removal

**Ensure removed fields are truly deleted:**
```lua
-- Not enough (field might still be in message)
msg["password"] = nil

-- Better: Explicitly set to empty/zero
msg["password"] = ""
msg["internal_id"] = 0
```

**Test Cases:**
```python
def test_sensitive_field_removal():
    """Verify password field is completely removed from response."""
    response = decode_grpc_response(...)

    # Check field is not in serialized message
    assert b"password" not in encode_grpc_message(response)

    # Check field is not accessible
    assert not hasattr(response, "password")
```

---

## 7. Test Strategy

### 7.1 Test Categories

#### 7.1.1 Unit Tests (tests/test_grpc_transformation.py)

1. **Config Model Tests (5 tests)**
   - test_proto_descriptor_validation
   - test_grpc_transformation_validation
   - test_route_grpc_transformation
   - test_config_proto_descriptors
   - test_invalid_source_type

2. **ProtoManager Tests (8 tests)**
   - test_proto_manager_init
   - test_register_file_descriptor
   - test_register_inline_descriptor
   - test_register_url_descriptor
   - test_compile_proto
   - test_missing_proto_file
   - test_invalid_proto_syntax
   - test_protoc_not_installed

3. **Provider Generation Tests (12 tests)**
   - test_envoy_lua_filter_generation
   - test_nginx_lua_block_generation
   - test_apisix_serverless_generation
   - test_kong_warning_message
   - test_haproxy_lua_reference
   - test_traefik_limitation_warning
   - test_envoy_request_transform_lua
   - test_envoy_response_transform_lua
   - test_nginx_field_add
   - test_nginx_field_remove
   - test_nginx_field_rename
   - test_grpc_header_preservation

4. **Integration Tests (5 tests)**
   - test_full_grpc_transformation_envoy
   - test_proto_hot_reload
   - test_large_message_handling
   - test_invalid_message_handling
   - test_multi_service_transformation

#### 7.1.2 Test Fixtures

```python
# fixtures/test.proto
syntax = "proto3";
package test.v1;

service TestService {
  rpc Echo(EchoRequest) returns (EchoResponse);
}

message EchoRequest {
  string message = 1;
  string secret = 2;
}

message EchoResponse {
  string message = 1;
  string timestamp = 2;
}
```

### 7.2 Mock Strategy

```python
# Mock protoc subprocess
@pytest.fixture
def mock_protoc(monkeypatch):
    def fake_run(cmd, **kwargs):
        # Simulate successful protoc compilation
        desc_file = cmd[1].split("=")[1]
        Path(desc_file).write_bytes(b"FAKE_DESC")
        return subprocess.CompletedProcess(cmd, 0, "", "")

    monkeypatch.setattr(subprocess, "run", fake_run)
```

### 7.3 Coverage Goals

- **Config Model**: 100% coverage
- **ProtoManager**: 95% coverage (exclude network timeouts)
- **Provider Generation**: 90% coverage
- **Overall**: 85% coverage

---

## 8. Deployment Strategies

### 8.1 Proto Descriptor Deployment

#### Option 1: Volume Mounts (Kubernetes)

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grpc-protos
data:
  user.proto: |
    syntax = "proto3";
    ...
---
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: gateway
        volumeMounts:
        - name: protos
          mountPath: /etc/gal/protos
      volumes:
      - name: protos
        configMap:
          name: grpc-protos
```

#### Option 2: Init Containers

```yaml
initContainers:
- name: proto-loader
  image: ghcr.io/pt9912/x-gal-proto-loader:latest
  command: ["sh", "-c"]
  args:
  - |
    # Download protos from artifact repository
    curl -o /protos/user.proto https://buf.build/myorg/user-api/proto

    # Compile to .desc
    protoc --descriptor_set_out=/protos/user.desc /protos/user.proto
  volumeMounts:
  - name: protos
    mountPath: /protos
```

#### Option 3: Inline in Config (Small Protos)

```yaml
proto_descriptors:
  - name: simple_service
    source: inline
    content: |
      syntax = "proto3";
      package simple.v1;
      service SimpleService {
        rpc Hello(HelloRequest) returns (HelloResponse);
      }
      message HelloRequest { string name = 1; }
      message HelloResponse { string greeting = 1; }
```

### 8.2 Lua Dependency Installation

#### Envoy (Docker)

```dockerfile
FROM envoyproxy/envoy:v1.28-latest

# Install lua-protobuf
RUN apk add --no-cache lua5.1-dev luarocks && \
    luarocks-5.1 install lua-protobuf

# Copy proto descriptors
COPY protos/*.desc /etc/gal/protos/
```

#### Nginx/OpenResty

```dockerfile
FROM openresty/openresty:alpine

# lua-protobuf already included in OpenResty
# Just copy protos
COPY protos/*.desc /etc/gal/protos/
```

#### APISIX

```bash
# APISIX includes lua-protobuf by default
docker exec apisix bash -c "
  cp /path/to/user.desc /usr/local/apisix/protos/
"
```

---

## 9. Backwards Compatibility

### 9.1 Config Format

**New fields are OPTIONAL:**
- Existing configs without `proto_descriptors` → continue working
- Existing routes without `grpc_transformation` → no change
- No breaking changes to existing config schema

**Example:**
```yaml
# Old config (still works)
services:
  - name: rest_api
    routes:
      - path_prefix: /api

# New config (with gRPC)
services:
  - name: grpc_api
    routes:
      - path_prefix: /api/v2
        grpc_transformation:
          enabled: true
          ...
```

### 9.2 Provider Output

**Providers without gRPC support:**
- Return None/empty config → no filters added
- Existing provider tests continue passing
- No regression in REST API handling

---

## 10. Documentation Structure

### 10.1 docs/guides/GRPC_TRANSFORMATIONS.md (1000+ Zeilen)

**Inhaltsverzeichnis:**

1. **Übersicht** (100 Zeilen)
   - Was sind gRPC Transformations?
   - Use Cases
   - Provider-Support-Matrix

2. **Schnellstart** (200 Zeilen)
   - Beispiel 1: Trace-ID Injection (Envoy)
   - Beispiel 2: Secret Removal (Nginx)
   - Beispiel 3: Field Renaming (APISIX)

3. **Proto Descriptor Management** (150 Zeilen)
   - File-based Descriptors
   - Inline Descriptors
   - URL-based Descriptors
   - Kompilierung mit protoc

4. **Konfigurationsoptionen** (200 Zeilen)
   - ProtoDescriptor-Felder
   - GrpcTransformation-Felder
   - RequestBodyTransformation
   - ResponseBodyTransformation
   - Vollständiges YAML-Schema

5. **Provider-Implementierungen** (250 Zeilen)
   - Envoy (Lua Filter Details)
   - Nginx (OpenResty Details)
   - APISIX (Serverless Plugin)
   - Kong (Alternatives)
   - HAProxy (Setup Instructions)
   - Traefik (Limitations)

6. **Deployment-Strategien** (100 Zeilen)
   - Kubernetes ConfigMaps
   - Docker Volume Mounts
   - Init Containers
   - Inline Configs

7. **Best Practices** (100 Zeilen)
   - Proto Versioning
   - Field Naming Conventions
   - Performance Optimization
   - Security Considerations

8. **Troubleshooting** (100 Zeilen)
   - Häufige Fehler
   - Debugging-Tipps
   - Log-Analyse

### 10.2 examples/grpc-transformation-example.yaml (10+ Szenarien)

**Szenarien:**

1. Trace-ID Injection
2. Secret Removal
3. Field Renaming
4. Response Filtering
5. Metadata Addition
6. Multiple Services
7. URL-based Proto
8. Inline Proto
9. Complex Transformations
10. Multi-Provider Config

---

## 11. Implementation Roadmap

### Phase 1: Woche 1-2 (Config Model + ProtoManager)

**Tasks:**
1. ✅ Config Model Extensions (ProtoDescriptor, GrpcTransformation)
2. ✅ ProtoManager Implementation
3. ✅ YAML Parsing Support
4. ✅ 5+ Config Tests
5. ✅ 8+ ProtoManager Tests

**Deliverables:**
- gal/config.py (updated)
- gal/proto_manager.py (new)
- tests/test_grpc_transformation.py (config + manager tests)

### Phase 2: Woche 2-3 (Provider Implementations)

**Tasks:**
1. ✅ Envoy Provider (Lua Filter)
2. ✅ Nginx Provider (OpenResty Lua)
3. ✅ APISIX Provider (Serverless)
4. ✅ Kong Provider (Warning)
5. ✅ HAProxy Provider (Instructions)
6. ✅ Traefik Provider (Limitation)
7. ✅ 12+ Provider Tests

**Deliverables:**
- gal/providers/envoy.py (updated)
- gal/providers/nginx.py (updated)
- gal/providers/apisix.py (updated)
- gal/providers/kong.py (updated)
- gal/providers/haproxy.py (updated)
- gal/providers/traefik.py (updated)
- tests/test_grpc_transformation.py (provider tests)

### Phase 3: Woche 3-4 (Documentation & Examples)

**Tasks:**
1. ✅ docs/guides/GRPC_TRANSFORMATIONS.md (1000+ Zeilen)
2. ✅ examples/grpc-transformation-example.yaml (10+ Szenarien)
3. ✅ README.md Updates
4. ✅ ROADMAP.md Updates
5. ✅ Integration Tests

**Deliverables:**
- docs/guides/GRPC_TRANSFORMATIONS.md (new)
- examples/grpc-transformation-example.yaml (new)
- README.md (updated)
- ROADMAP.md (updated)
- tests/test_grpc_transformation.py (integration tests)

---

## 12. Success Criteria

### 12.1 Functional Requirements

- ✅ Config model supports all 3 proto sources (file, inline, url)
- ✅ ProtoManager compiles .proto → .desc
- ✅ Envoy generates valid Lua filter
- ✅ Nginx generates valid OpenResty Lua blocks
- ✅ APISIX generates valid serverless config
- ✅ Kong shows helpful alternatives
- ✅ HAProxy provides setup instructions
- ✅ Traefik clearly states limitations

### 12.2 Quality Requirements

- ✅ 30+ Tests passing (100% success rate)
- ✅ 85%+ Code coverage
- ✅ All edge cases handled
- ✅ No security vulnerabilities
- ✅ Performance overhead < 30µs per request

### 12.3 Documentation Requirements

- ✅ 1000+ Zeilen deutsche Dokumentation
- ✅ 10+ Beispiel-Szenarien
- ✅ README.md updated
- ✅ ROADMAP.md updated

---

## 13. Open Questions

1. **Proto Schema Evolution:**
   - Wie gehen wir mit Breaking Changes um?
   - Versioning-Strategie für Descriptors?
   - → **Antwort:** User-Verantwortung, Dokumentation zu Proto-Versioning

2. **gRPC Streaming:**
   - Unterstützung für Server/Client/Bidirectional Streaming?
   - → **Antwort:** v1.4.0 nur Unary RPCs, Streaming in v1.5.0

3. **Performance Benchmarks:**
   - Real-world Performance-Tests mit load testing?
   - → **Antwort:** Dokumentation mit erwarteten Overhead-Zahlen

4. **Proto Reflection:**
   - Dynamische Proto-Discovery via gRPC Reflection API?
   - → **Antwort:** Out of scope for v1.4.0, File-based descriptors nur

---

## 14. Zusammenfassung

Diese Spezifikation definiert ein vollständiges System für gRPC-Transformationen in GAL:

- **3 neue Config-Klassen** (ProtoDescriptor, GrpcTransformation, + Route/Config Extensions)
- **1 neues Modul** (ProtoManager)
- **6 Provider-Updates** (Envoy, Nginx, APISIX, Kong, HAProxy, Traefik)
- **30+ Tests** (Config, Manager, Provider, Integration)
- **1000+ Zeilen Dokumentation**
- **10+ Beispiele**

**Implementierungszeit:** 3-4 Wochen
**Codezeilen:** ~2000 neue Zeilen
**Aufwand:** Hoch (aber wohlspezifiziert)

Die Spezifikation ist bereit für die Implementierung!
